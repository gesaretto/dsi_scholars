---
title: "Notes about section 2 of Propublica's piece on COMPAS"
author: G. E. Saretto
---

### How we acquired the Data

- Proquest obtained data from Broward County Florida. For the study, they used both the results of the COMPAS tests completed in the county over the two years 2013-2014, and the criminal records made publicly available by the County from 2013 until 2016. There are some minor issues in the choices that led to their selection and handling of data, and in the phrasing that they deploy to explain these choices. First, they refer to COMPAS as to "one of the most popular scores used nationwide," but they do not specify how extensively it is used, and by how much it is "increasingly" used in pretrial and sentencing. Moreover, they do not provide much justification for why they decided to concentrate on this particular county; they argue that it is "larger" compared with other counties that use COMPAS similarly--but by how much? And what social and demographic factors could affect their results, when compared with the results that could be obtained from the same sort of inquiry in some other region of the US? They also claim that COMPAS is "primarily" used to determine whether to release or detain a defendant, but they do not explain how often, and by what extent, when compared with other uses of the software. This last generalization actually leads them to make a decision that would have definitely deserved a more thorough rationale; out of the 18,610 score results they decided to discard 6,853, on the basis that those scores were assessed "at different stages in the criminal justice system"--that is, not during pretrial. But, if the goal is to assess the validity of the scoring system as a whole, why excluding so many relevant cases? And, again, what sort of numbers justify this decision?

The authors then break down the scoring system used by COMPAS. Afterwards, they explain how they matched the scores with the public criminal records. Now, it should be noted that they had to match these themselves; the county apparently did not provide them with information about the actual context in which these scores were obtained. The matching process led to a marginal percent of error which might be discussed further (__What does "CI" mean?__).

### How we defined recidivism

- The definition of recidivism was apparently adapted from the one adopted by the same company which developed the software--Northpointe. It should be noted, however, that COMPAS calls recidivism a second "arrest involving a charge and a filing" (after the date when the test was taken), while ProPublica speaks of "jail booking." Do these two circumstances perfectly overlap? (__I actually do not know__; does every arrest involve a "jail booking"? Are there arrests that instead occur just in a police station?)

> Definition of "booking" from MW: a procedure at a jail or police station following an arrest in which information about the arrest (as the time, the name of the arrested person, and the crime for which the arrest was made) is entered in the police register).

- What follows is fuzzy. The question is timing. The researchers at ProPublica need to assess whether the crime which makes the defendant a "recidive" (the second crime) has been committed _before or after_ the crime which caused the taking of the COMPAS test. Now, one method would be that of simply referring to two dates--the date of when the test was taken, and that of when the crime was perpetrated. Every crime that occurs _after_ the first date, the taking of the test, would count as recidivism. (Because, one would argue, the test would _inevitably_ happen after the first arrest.) However, ProPublica resorts to a more intricate method; they try to establish what the first crime was by considering the arrests made within 30 days of the taking of the test, and use the date of that arrest as their first date. This led them to "remove" even more cases from their data set.

- ProPublica adds some further clarifications. They specify that they did not consider "recidivism" traffic tickets and "some municipal ordinance violations"; but they do not provide more specific violations, nor explain whether COMPAS considered these transgressions in their own algorithm. The same could be said about their definition of violent recidivism; they adopt the one proposed by the FBI, but they do not explain whether COMPAS did the same. Shouldn't these differences be taken into account, if the goal is to test the accuracy of the software system?
